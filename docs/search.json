[
  {
    "objectID": "06_synthesizer.html",
    "href": "06_synthesizer.html",
    "title": "python applied machine learning",
    "section": "",
    "text": "!uv run deepeval set-ollama deepseek-r1:8b\n!uv run deepeval set-local-embeddings --model-name=mxbai-embed-large --base-url=\"http://localhost:11434/v1/\"  --api-key=\"ollama\"\n\nüôå Congratulations! You're now using a local Ollama model for all evals that \nrequire an LLM.\nüôå Congratulations! You're now using local embeddings for all evals that require\ntext embeddings.\n\n\n\nfrom deepeval.dataset import EvaluationDataset\nfrom deepeval.synthesizer import Synthesizer\nfrom deepeval.synthesizer.config import ContextConstructionConfig\n\nsynthesizer = Synthesizer()\ngoldens = synthesizer.generate_goldens_from_docs(\n    document_paths=[\"example.txt\"],  # , \"example.docx\", \"example.pdf\"]\n    context_construction_config=ContextConstructionConfig(chunk_size=200),\n)\n\ndataset = EvaluationDataset(goldens=goldens)\n\n‚ú® üöÄ ‚ú® Loading Documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 11.55it/s]\n‚ú® üìö ‚ú® Chunking Documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00,  2.51it/s]\n‚ú® üß© ‚ú® Generating Contexts:   0%|                                                                                | 0/9 [00:00&lt;?, ?it/s]\n  ‚ú® ü´ó ‚ú® Filling Contexts:   0%|                                                                                 | 0/6 [00:00&lt;?, ?it/s]\n  ‚ú® ü´ó ‚ú® Filling Contexts:  17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                            | 1/6 [00:12&lt;01:04, 12.85s/it]\n  ‚ú® ü´ó ‚ú® Filling Contexts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                    | 3/6 [00:12&lt;00:10,  3.38s/it]\n‚ú® üß© ‚ú® Generating Contexts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:13&lt;00:00,  1.45s/it]  \n\n\nNot enough available chunks in smallest document to evaluate chunk quality using the filter threshold: 0.5.\n\n\nUtilizing 4 out of 4 chunks.\n\n\n\n‚ú® Generating up to 6 goldens using DeepEval (using deepseek-r1:8b (Ollama) and local embeddings, method=docs):   0%| | 0/6 [00:14&lt;?, ?it\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[2], line 6\n      3 from deepeval.synthesizer.config import ContextConstructionConfig\n      5 synthesizer = Synthesizer()\n----&gt; 6 goldens = synthesizer.generate_goldens_from_docs(\n      7     document_paths=[\"example.txt\"],  # , \"example.docx\", \"example.pdf\"]\n      8     context_construction_config=ContextConstructionConfig(chunk_size=200),\n      9 )\n     11 dataset = EvaluationDataset(goldens=goldens)\n\nFile ~/Documents/go/python-llm-eval/.venv/lib/python3.12/site-packages/deepeval/synthesizer/synthesizer.py:131, in Synthesizer.generate_goldens_from_docs(self, document_paths, include_expected_output, max_goldens_per_context, context_construction_config, _send_data)\n    129 if self.async_mode:\n    130     loop = get_or_create_event_loop()\n--&gt; 131     goldens = loop.run_until_complete(\n    132         self.a_generate_goldens_from_docs(\n    133             document_paths=document_paths,\n    134             include_expected_output=include_expected_output,\n    135             max_goldens_per_context=max_goldens_per_context,\n    136             context_construction_config=context_construction_config,\n    137             _reset_cost=False,\n    138         )\n    139     )\n    140 else:\n    141     # Generate contexts from provided docs\n    142     context_generator = ContextGenerator(\n    143         document_paths=document_paths,\n    144         embedder=context_construction_config.embedder,\n   (...)    150         max_retries=context_construction_config.max_retries,\n    151     )\n\nFile ~/Documents/go/python-llm-eval/.venv/lib/python3.12/site-packages/nest_asyncio.py:98, in _patch_loop.&lt;locals&gt;.run_until_complete(self, future)\n     95 if not f.done():\n     96     raise RuntimeError(\n     97         'Event loop stopped before Future completed.')\n---&gt; 98 return f.result()\n\nFile ~/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/futures.py:202, in Future.result(self)\n    200 self.__log_traceback = False\n    201 if self._exception is not None:\n--&gt; 202     raise self._exception.with_traceback(self._exception_tb)\n    203 return self._result\n\nFile ~/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:316, in Task.__step_run_and_handle_result(***failed resolving arguments***)\n    314         result = coro.send(None)\n    315     else:\n--&gt; 316         result = coro.throw(exc)\n    317 except StopIteration as exc:\n    318     if self._must_cancel:\n    319         # Task is cancelled right before coro stops.\n\nFile ~/Documents/go/python-llm-eval/.venv/lib/python3.12/site-packages/deepeval/synthesizer/synthesizer.py:237, in Synthesizer.a_generate_goldens_from_docs(self, document_paths, include_expected_output, max_goldens_per_context, context_construction_config, _reset_cost)\n    228 # Generate goldens from generated contexts\n    229 with synthesizer_progress_context(\n    230     method=\"docs\",\n    231     num_evolutions=self.evolution_config.num_evolutions,\n   (...)    235     max_generations=len(contexts) * max_goldens_per_context,\n    236 ) as progress_bar:\n--&gt; 237     goldens = await self.a_generate_goldens_from_contexts(\n    238         contexts=contexts,\n    239         include_expected_output=include_expected_output,\n    240         max_goldens_per_context=max_goldens_per_context,\n    241         source_files=source_files,\n    242         _context_scores=context_scores,\n    243         _progress_bar=progress_bar,\n    244         _reset_cost=False,\n    245     )\n    246 self.synthetic_goldens.extend(goldens)\n    247 if _reset_cost and self.cost_tracking and self.using_native_model:\n\nFile ~/Documents/go/python-llm-eval/.venv/lib/python3.12/site-packages/deepeval/synthesizer/synthesizer.py:420, in Synthesizer.a_generate_goldens_from_contexts(self, contexts, include_expected_output, max_goldens_per_context, source_files, _context_scores, _progress_bar, _reset_cost)\n    395 with synthesizer_progress_context(\n    396     method=\"default\",\n    397     num_evolutions=self.evolution_config.num_evolutions,\n   (...)    403     async_mode=True,\n    404 ) as progress_bar:\n    405     tasks = [\n    406         self.task_wrapper(\n    407             semaphore,\n   (...)    418         for index, context in enumerate(contexts)\n    419     ]\n--&gt; 420     await asyncio.gather(*tasks)\n    422 if _reset_cost and self.cost_tracking and self.using_native_model:\n    423     print(f\"üí∞ API cost: {self.synthesis_cost:.6f}\")\n\nFile ~/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:385, in Task.__wakeup(self, future)\n    383 def __wakeup(self, future):\n    384     try:\n--&gt; 385         future.result()\n    386     except BaseException as exc:\n    387         # This may also be a cancellation.\n    388         self.__step(exc)\n\nFile ~/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:314, in Task.__step_run_and_handle_result(***failed resolving arguments***)\n    310 try:\n    311     if exc is None:\n    312         # We use the `send` method directly, because coroutines\n    313         # don't have `__iter__` and `__next__` methods.\n--&gt; 314         result = coro.send(None)\n    315     else:\n    316         result = coro.throw(exc)\n\nFile ~/Documents/go/python-llm-eval/.venv/lib/python3.12/site-packages/deepeval/synthesizer/synthesizer.py:1005, in Synthesizer.task_wrapper(self, sem, func, *args, **kwargs)\n   1003 async def task_wrapper(self, sem, func, *args, **kwargs):\n   1004     async with sem:  # Acquire semaphore\n-&gt; 1005         return await func(*args, **kwargs)\n\nFile ~/Documents/go/python-llm-eval/.venv/lib/python3.12/site-packages/deepeval/synthesizer/synthesizer.py:445, in Synthesizer._a_generate_from_context(self, context, goldens, include_expected_output, max_goldens_per_context, source_files, index, progress_bar, context_scores)\n    426 async def _a_generate_from_context(\n    427     self,\n    428     context: List[str],\n   (...)    436 ):\n    437     # Generate inputs\n    438     prompt = SynthesizerTemplate.generate_synthetic_inputs(\n    439         context=context,\n    440         max_goldens_per_context=max_goldens_per_context,\n   (...)    443         input_format=self.styling_config.input_format,\n    444     )\n--&gt; 445     synthetic_inputs: List[SyntheticData] = await self._a_generate_inputs(\n    446         prompt\n    447     )\n    449     # Qualify inputs\n    450     qualified_synthetic_inputs: List[SyntheticData]\n\nFile ~/Documents/go/python-llm-eval/.venv/lib/python3.12/site-packages/deepeval/synthesizer/synthesizer.py:722, in Synthesizer._a_generate_inputs(self, prompt)\n    718 async def _a_generate_inputs(self, prompt: str) -&gt; List[SyntheticData]:\n    719     res: SyntheticDataList = await self._a_generate_schema(\n    720         prompt, SyntheticDataList, self.model\n    721     )\n--&gt; 722     synthetic_data_items = res.data\n    723     return synthetic_data_items\n\nAttributeError: 'tuple' object has no attribute 'data'\n\n\n\n\nsynthesizer.to_pandas()",
    "crumbs": [
      "06 Synthesizer"
    ]
  },
  {
    "objectID": "04_deepeval.html",
    "href": "04_deepeval.html",
    "title": "python applied machine learning",
    "section": "",
    "text": "!uv run deepeval set-ollama deepseek-r1:8b\n\nüôå Congratulations! You're now using a local Ollama model for all evals that \nrequire an LLM.\n\n\n\n!cat .deepeval\n\n{\"LOCAL_MODEL_NAME\": \"deepseek-r1:8b\", \"LOCAL_MODEL_BASE_URL\": \"http://localhost:11434\", \"USE_LOCAL_MODEL\": \"YES\", \"USE_AZURE_OPENAI\": \"NO\", \"LOCAL_MODEL_API_KEY\": \"ollama\"}\n\n\n\nfrom deepeval import evaluate\nfrom deepeval.dataset import EvaluationDataset\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCase, LLMTestCaseParams\n\ncorrectness_metric = GEval(\n    name=\"Correctness\",\n    criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\",\n    evaluation_params=[\n        LLMTestCaseParams.ACTUAL_OUTPUT,\n        LLMTestCaseParams.EXPECTED_OUTPUT,\n    ],\n    threshold=0.5,\n)\ntest_case = LLMTestCase(\n    input=\"I have a persistent cough and fever. Should I be worried?\",\n    # Replace this with the actual output of your LLM application\n    actual_output=\"A persistent cough and fever could be a viral infection or something more serious. See a doctor if symptoms worsen or don‚Äôt improve in a few days.\",\n    expected_output=\"A persistent cough and fever could indicate a range of illnesses, from a mild viral infection to more serious conditions like pneumonia or COVID-19. You should seek medical attention if your symptoms worsen, persist for more than a few days, or are accompanied by difficulty breathing, chest pain, or other concerning signs.\",\n)\n\ndataset = EvaluationDataset(test_cases=[test_case])\nevaluate(dataset, [correctness_metric])\n\n‚ú® You're running DeepEval's latest Correctness (GEval) Metric! (using deepseek-r1:8b (Ollama), strict=False, \nasync_mode=True)...\n\n\n\nEvaluating 1 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (1/1) [Time Taken: 00:42, 42.55s/test case]\n\n\n\n======================================================================\n\nMetrics Summary\n\n  - ‚úÖ Correctness (GEval) (score: 0.7, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The actual output acknowledges that persistent cough and fever could be serious but does not specify conditions like pneumonia or COVID-19. It also advises seeing a doctor if symptoms worsen or improve within a few days, which is less detailed than the expected output., error: None)\n\nFor test case:\n\n  - input: I have a persistent cough and fever. Should I be worried?\n  - actual output: A persistent cough and fever could be a viral infection or something more serious. See a doctor if symptoms worsen or don‚Äôt improve in a few days.\n  - expected output: A persistent cough and fever could indicate a range of illnesses, from a mild viral infection to more serious conditions like pneumonia or COVID-19. You should seek medical attention if your symptoms worsen, persist for more than a few days, or are accompanied by difficulty breathing, chest pain, or other concerning signs.\n  - context: None\n  - retrieval context: None\n\n======================================================================\n\nOverall Metric Pass Rates\n\nCorrectness (GEval): 100.00% pass rate\n\n======================================================================\n\n\n\n\n\n\n‚úì Tests finished üéâ! Run 'deepeval login' to save and analyze evaluation results on Confident AI.\n \n‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use Confident AI to get & share testing reports, \nexperiment with models/prompts, and catch regressions for your LLM system. Just run 'deepeval login' in the CLI. \n\n\n\n\nEvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Correctness (GEval)', threshold=0.5, success=True, score=0.7, reason='The actual output acknowledges that persistent cough and fever could be serious but does not specify conditions like pneumonia or COVID-19. It also advises seeing a doctor if symptoms worsen or improve within a few days, which is less detailed than the expected output.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Criteria:\\nDetermine if the \\'actual output\\' is correct based on the \\'expected output\\'. \\n \\nEvaluation Steps:\\n[\\n    \"Compare each aspect of the actual output with the corresponding expected output criterion.\",\\n    \"Evaluate whether the actual output meets the key requirements outlined in the expected output.\",\\n    \"Assess the quality, clarity, and effectiveness of the actual output in alignment with the criteria.\",\\n    \"Identify any deviations from the expected output and determine their impact on correctness.\"\\n]')], conversational=False, multimodal=False, input='I have a persistent cough and fever. Should I be worried?', actual_output='A persistent cough and fever could be a viral infection or something more serious. See a doctor if symptoms worsen or don‚Äôt improve in a few days.', expected_output='A persistent cough and fever could indicate a range of illnesses, from a mild viral infection to more serious conditions like pneumonia or COVID-19. You should seek medical attention if your symptoms worsen, persist for more than a few days, or are accompanied by difficulty breathing, chest pain, or other concerning signs.', context=None, retrieval_context=None)], confident_link=None)",
    "crumbs": [
      "04 Deepeval"
    ]
  },
  {
    "objectID": "02-evaluate.html",
    "href": "02-evaluate.html",
    "title": "Combining several evaluations",
    "section": "",
    "text": "import evaluate\naccuracy = evaluate.load('accuracy')\nprint(accuracy.description)\n\n\nAccuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\nAccuracy = (TP + TN) / (TP + TN + FP + FN)\n Where:\nTP: True positive\nTN: True negative\nFP: False positive\nFN: False negative\nprint(accuracy.features)\n\n{'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}\npredictions = [1, 1, 1, 1]\nreferences = [1, 1, 0, 1]\n\naccuracy.compute(predictions=predictions, references=references)\n\n{'accuracy': 0.75}\nfor pred, ref in zip(predictions, references):\n    accuracy.add(predictions=pred, references=ref)\naccuracy.compute()\n\n{'accuracy': 0.75}\nfor pred, ref in zip([[1, 1], [1, 1]], [[1, 1], [0, 1]]):\n    accuracy.add_batch(predictions=pred, references=ref)\naccuracy.compute()\n\n{'accuracy': 0.75}\nclf_metrics = evaluate.combine(['accuracy', 'f1', 'precision', 'recall'])\nclf_metrics.compute(predictions=predictions, references=references)\n\n{'accuracy': 0.75, 'f1': 0.8571428571428571, 'precision': 0.75, 'recall': 1.0}",
    "crumbs": [
      "Combining several evaluations"
    ]
  },
  {
    "objectID": "02-evaluate.html#rouge",
    "href": "02-evaluate.html#rouge",
    "title": "Combining several evaluations",
    "section": "1 Rouge",
    "text": "1 Rouge\nhttps://huggingface.co/spaces/evaluate-metric/rouge\n\nrouge = evaluate.load('rouge')\nprint(rouge.description)\n\nROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for\nevaluating automatic summarization and machine translation software in natural language processing.\nThe metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\n\nNote that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters.\n\nThis metrics is a wrapper around Google Research reimplementation of ROUGE:\nhttps://github.com/google-research/google-research/tree/master/rouge\n\n\n\n\nprint(rouge.features)\n\n[{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}]\n\n\n\nrouge.compute(predictions=['hello john', 'I am one'], \n              references=['hello John', 'I am two'])\n\n{'rouge1': 0.8333333333333333,\n 'rouge2': 0.75,\n 'rougeL': 0.8333333333333333,\n 'rougeLsum': 0.8333333333333333}\n\n\n\nrouge.compute(predictions=['the cat sat on the mat'], \n              references=['the cat was on the mat'])\n\n{'rouge1': 0.8333333333333334,\n 'rouge2': 0.6,\n 'rougeL': 0.8333333333333334,\n 'rougeLsum': 0.8333333333333334}",
    "crumbs": [
      "Combining several evaluations"
    ]
  },
  {
    "objectID": "02-evaluate.html#bleu",
    "href": "02-evaluate.html#bleu",
    "title": "Combining several evaluations",
    "section": "2 BLEU",
    "text": "2 BLEU\nhttps://huggingface.co/spaces/evaluate-metric/bleu\n\nbleu = evaluate.load('bleu')\nprint(bleu.description)\n\nDownloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.94k/5.94k [00:00&lt;00:00, 5.88MB/s]\nDownloading extra modules: 4.07kB [00:00, 7.39MB/s]                                                                                                    \nDownloading extra modules: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.34k/3.34k [00:00&lt;00:00, 10.3MB/s]\n\n\nBLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another.\nQuality is considered to be the correspondence between a machine's output and that of a human: \"the closer a machine translation is to a professional human translation, the better it is\"\n‚Äì this is the central idea behind BLEU. BLEU was one of the first metrics to claim a high correlation with human judgements of quality, and remains one of the most popular automated and inexpensive metrics.\n\nScores are calculated for individual translated segments‚Äîgenerally sentences‚Äîby comparing them with a set of good quality reference translations.\nThose scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality.\nNeither intelligibility nor grammatical correctness are not taken into account.\n\n\n\n\n\n\n\nprint(bleu.features)\n\n[{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}]\n\n\n\nbleu.compute(predictions=['hello john', 'I am one'], \n             references=['hello John', 'I am two'])\n\n{'bleu': 0.0,\n 'precisions': [0.6, 0.3333333333333333, 0.0, 0.0],\n 'brevity_penalty': 1.0,\n 'length_ratio': 1.0,\n 'translation_length': 5,\n 'reference_length': 5}\n\n\n\nbleu.compute(predictions=['the cat sat on the mat'], \n             references=['the cat was on the mat'])\n\n{'bleu': 0.0,\n 'precisions': [0.8333333333333334, 0.6, 0.25, 0.0],\n 'brevity_penalty': 1.0,\n 'length_ratio': 1.0,\n 'translation_length': 6,\n 'reference_length': 6}",
    "crumbs": [
      "Combining several evaluations"
    ]
  },
  {
    "objectID": "01-eval.html",
    "href": "01-eval.html",
    "title": "Evaluating LLM Performance",
    "section": "",
    "text": "To effectively evaluate LLM performance, we need to consider various aspects beyond just accuracy. These aspects include fluency, coherence, relevance, diversity, factual accuracy, and the ability to generate meaningful responses.\n\nLLM Benchmarks: Example: SuperGLUE, a suite of benchmarks for evaluating LLMs on diverse natural language tasks.\nMetrics: Example: BLEU score, which measures the similarity between generated text and human-written references.\nHuman Evaluation: Example: Collect human judgments on the quality of generated text, such as fluency, coherence, and informativeness.\nMulti-Task Evaluation: Example: Evaluate an LLM on a variety of tasks to assess its overall effectiveness.\nExplainability Methods: Example: Utilize techniques such as attention visualization to understand how an LLM generates text and identify potential biases.\n\nBLEU and ROUGE scores are metrics commonly used to evaluate the performance of natural language processing (NLP) models, specifically machine translation and text summarization models.\n\n0.1 BLEU (Bilingual Evaluation Understudy):\n\nMeasures the similarity between a machine-generated text and human-written reference translations.\nFocuses on precision, meaning it penalizes the model for generating words not present in the reference translations.\nCalculates n-gram matches (overlapping sequences of words) between the generated and reference texts.\nRanges from 0 (no overlap) to 1 (perfect match).\n\n\n\n0.2 ROUGE (Recall-Oriented Understudy for Gisting Evaluation):\n\nMeasures the overlap between machine-generated summaries and human-written reference summaries.\nFocuses on recall, meaning it rewards the model for capturing important information from the original text.\nUtilizes various n-gram sizes to capture different levels of granularity.\nOffers several variants, including ROUGE-N (n-gram overlap), ROUGE-L (longest common subsequence), and ROUGE-W (weighted n-gram overlap).\nScores are reported as F1 scores, which combine precision and recall.\n\nBoth BLEU and ROUGE scores offer valuable insights into the performance of NLP models, but they have limitations. BLEU can be sensitive to word order and may not capture fluency or coherence. ROUGE can reward summaries that are overly repetitive or lack originality.\nHere are some additional points to consider:\n\nBLEU is often used for machine translation, while ROUGE is preferred for text summarization.\nNewer metrics are being developed to address the limitations of BLEU and ROUGE, such as METEOR and CHRF++.\nHuman evaluation is still considered the gold standard for NLP model evaluation, but it can be expensive and time-consuming.\n\nNo single metric can perfectly capture the quality of a machine-generated text. It‚Äôs important to consider the specific task and application when choosing the most appropriate evaluation methods.",
    "crumbs": [
      "Evaluating LLM Performance"
    ]
  },
  {
    "objectID": "03-llm.html",
    "href": "03-llm.html",
    "title": "Evaluating LLM using LLM",
    "section": "",
    "text": "LLMs can be used to evaluate other LLMs by leveraging their ability to generate text, translate languages, and answer questions. This approach offers several potential benefits, including:\n\nScalability: LLMs can quickly evaluate large datasets of text, overcoming the time and resource limitations of human evaluation.\nObjectivity: LLMs can be less prone to bias than human evaluators, potentially leading to more consistent and fair assessments.\nFlexibility: LLMs can be tailored to specific evaluation tasks and criteria by providing appropriate prompts and instructions.\n\nHere are some sample prompts for evaluating an LLM using another LLM:\n\n0.1 1. Fluency and Coherence:\n\nPrompt: ‚ÄúRead the following text and rate its fluency and coherence on a scale of 1 to 5. Provide specific feedback on areas for improvement.‚Äù\nExample: ‚ÄúThe cat sat on the mat. The dog chased the ball. The sun was shining brightly.‚Äù\n\n\n\n0.2 2. Relevance and Informativeness:\n\nPrompt: ‚ÄúWrite a summary of the following text and assess its relevance and informativeness. Does it capture the key points of the original text effectively?‚Äù\nExample: ‚ÄúThe article discusses the history of artificial intelligence and its potential impact on society.‚Äù\n\n\n\n0.3 3. Factual Accuracy and Credibility:\n\nPrompt: ‚ÄúVerify the factual accuracy of the following claims and indicate any sources that support your findings.‚Äù\nExample: ‚ÄúClimate change is caused by human activity.‚Äù\n\n\n\n0.4 4. Originality and Creativity:\n\nPrompt: ‚ÄúGenerate a creative text format based on the following prompt, demonstrating originality and effective use of language.‚Äù\nExample: ‚ÄúWrite a poem about a lost love.‚Äù\n\n\n\n0.5 5. Societal and Ethical Considerations:\n\nPrompt: ‚ÄúAnalyze the following text for potential biases or harmful stereotypes. Suggest ways to mitigate these issues.‚Äù\nExample: ‚ÄúA news article reporting on a crime that uses discriminatory language.‚Äù",
    "crumbs": [
      "Evaluating LLM using LLM"
    ]
  },
  {
    "objectID": "05_dag.html",
    "href": "05_dag.html",
    "title": "python applied machine learning",
    "section": "",
    "text": "from deepeval import evaluate\nfrom deepeval.dataset import EvaluationDataset\nfrom deepeval.metrics import DAGMetric\nfrom deepeval.metrics.dag import (\n    BinaryJudgementNode,\n    DeepAcyclicGraph,\n    NonBinaryJudgementNode,\n    TaskNode,\n    VerdictNode,\n)\nfrom deepeval.test_case import LLMTestCase, LLMTestCaseParams\n\ncorrect_order_node = NonBinaryJudgementNode(\n    criteria=\"Are the summary headings in the correct order: 'intro' =&gt; 'body' =&gt; 'conclusion'?\",\n    children=[\n        VerdictNode(verdict=\"Yes\", score=10),\n        VerdictNode(verdict=\"Two are out of order\", score=4),\n        VerdictNode(verdict=\"All out of order\", score=2),\n    ],\n)\n\ncorrect_headings_node = BinaryJudgementNode(\n    criteria=\"Does the summary headings contain all three: 'intro', 'body', and 'conclusion'?\",\n    children=[\n        VerdictNode(verdict=False, score=0),\n        VerdictNode(verdict=True, child=correct_order_node),\n    ],\n)\n\nextract_headings_node = TaskNode(\n    instructions=\"Extract all headings in `actual_output`\",\n    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n    output_label=\"Summary headings\",\n    children=[correct_headings_node, correct_order_node],\n)\n\n# Initialize the DAG\ndag = DeepAcyclicGraph(root_nodes=[extract_headings_node])\n\n# Create metric!\nmetric = DAGMetric(name=\"Summarization\", dag=dag)\n\n\nfrom ollama import ChatResponse, chat\n\n\ndef query(prompt: str) -&gt; str:\n    response: ChatResponse = chat(\n        model=\"llama3.2\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": prompt,\n            },\n        ],\n    )\n    return response.message.content\n\n\nprompt = \"provide a summary about cats. keep it in a paragraph, provide intro, body and conclusion\"\nresponse = query(prompt)\nprint(response)\n\n**Introduction**\n\nCats are one of the most popular pets worldwide, known for their independence, agility, and affectionate nature. With over 70 recognized breeds, cats have been human companions for thousands of years, providing entertainment, comfort, and loyalty to their owners. From ancient Egyptian worshippers to modern-day cat enthusiasts, felines have captured the hearts of people across cultures and generations.\n\n**Body**\n\nCats are characterized by their unique physical features, including their slender bodies, short legs, and retractable claws. Their soft, fur coats come in a wide range of colors and patterns, making each individual cat distinct. Domesticated cats are known for their playful, curious nature, often exhibiting behaviors such as pouncing on toys, scratching furniture, and kneading with their paws. Cats also possess exceptional hearing and vision abilities, allowing them to navigate and hunt with ease in the dark. Despite their independent reputation, many cats form strong bonds with their owners, enjoying cuddles, playtime, and even sharing meals.\n\n**Conclusion**\n\nCats continue to fascinate and enchant people around the world, offering a unique combination of independence and affection. Whether as pets, companions, or simply admired for their beauty and agility, cats have earned their place as beloved animals in human society. With their rich history, diverse breeds, and endearing personalities, it's no wonder that cats remain one of the most popular pets globally, bringing joy and companionship to millions of people worldwide.\n\n\n\ndataset = EvaluationDataset(\n    test_cases=[LLMTestCase(input=prompt, actual_output=response)]\n)\n\n\nevaluate(dataset, [metric])\n\n‚ú® You're running DeepEval's latest Summarization (DAG) Metric! (using deepseek-r1:8b (Ollama), strict=False, \nasync_mode=True)...\n\n\n\nEvaluating 1 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (1/1) [Time Taken: 00:10, 10.50s/test case]\n\n\n\n======================================================================\n\nMetrics Summary\n\n  - ‚úÖ Summarization (DAG) (score: 1.0, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 1.0 because all summary headings are correctly structured and follow the required order., error: None)\n\nFor test case:\n\n  - input: provide a summary about cats. keep it in a paragraph, provide intro, body and conclusion\n  - actual output: **Introduction**\n\nCats are one of the most popular pets worldwide, known for their independence, agility, and affectionate nature. With over 70 recognized breeds, cats have been human companions for thousands of years, providing entertainment, comfort, and loyalty to their owners. From ancient Egyptian worshippers to modern-day cat enthusiasts, felines have captured the hearts of people across cultures and generations.\n\n**Body**\n\nCats are characterized by their unique physical features, including their slender bodies, short legs, and retractable claws. Their soft, fur coats come in a wide range of colors and patterns, making each individual cat distinct. Domesticated cats are known for their playful, curious nature, often exhibiting behaviors such as pouncing on toys, scratching furniture, and kneading with their paws. Cats also possess exceptional hearing and vision abilities, allowing them to navigate and hunt with ease in the dark. Despite their independent reputation, many cats form strong bonds with their owners, enjoying cuddles, playtime, and even sharing meals.\n\n**Conclusion**\n\nCats continue to fascinate and enchant people around the world, offering a unique combination of independence and affection. Whether as pets, companions, or simply admired for their beauty and agility, cats have earned their place as beloved animals in human society. With their rich history, diverse breeds, and endearing personalities, it's no wonder that cats remain one of the most popular pets globally, bringing joy and companionship to millions of people worldwide.\n  - expected output: None\n  - context: None\n  - retrieval context: None\n\n======================================================================\n\nOverall Metric Pass Rates\n\nSummarization (DAG): 100.00% pass rate\n\n======================================================================\n\n\n\n\n\n\n‚úì Tests finished üéâ! Run 'deepeval login' to save and analyze evaluation results on Confident AI.\n \n‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use Confident AI to get & share testing reports, \nexperiment with models/prompts, and catch regressions for your LLM system. Just run 'deepeval login' in the CLI. \n\n\n\n\nEvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Summarization (DAG)', threshold=0.5, success=True, score=1.0, reason='The score is 1.0 because all summary headings are correctly structured and follow the required order.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs=\"______________________\\n| TaskNode | Level == 0 |\\n*******************************\\nLabel: None\\n\\nInstructions:\\nExtract all headings in `actual_output`\\n\\nSummary headings:\\nCats are one of the most popular pets worldwide\\n \\n \\n__________________________________\\n| BinaryJudgementNode | Level == 1 |\\n************************************************\\nLabel: None\\n\\nCriteria:\\nDoes the summary headings contain all three: 'intro', 'body', and 'conclusion'?\\n\\nVerdict: True\\nReason: The structure contains all three sections: introduction, body, and conclusion.\\n \\n \\n_____________________________________\\n| NonBinaryJudgementNode | Level == 2 |\\n*****************************************************\\nLabel: None\\n\\nCriteria:\\nAre the summary headings in the correct order: 'intro' =&gt; 'body' =&gt; 'conclusion'?\\n\\nVerdict: Yes\\nReason: The correct order for summary headings is intro, body, and conclusion. The given headings are Cats are one of the most popular pets worldwide (which should be an intro), followed by [empty], then [empty]. This skips the necessary body section before the conclusion. Therefore, the correct order is not followed.\\n \\n \\n________________________\\n| VerdictNode | Level == 3 |\\n**********************************\\nVerdict: Yes\\nType: Deterministic\")], conversational=False, multimodal=False, input='provide a summary about cats. keep it in a paragraph, provide intro, body and conclusion', actual_output=\"**Introduction**\\n\\nCats are one of the most popular pets worldwide, known for their independence, agility, and affectionate nature. With over 70 recognized breeds, cats have been human companions for thousands of years, providing entertainment, comfort, and loyalty to their owners. From ancient Egyptian worshippers to modern-day cat enthusiasts, felines have captured the hearts of people across cultures and generations.\\n\\n**Body**\\n\\nCats are characterized by their unique physical features, including their slender bodies, short legs, and retractable claws. Their soft, fur coats come in a wide range of colors and patterns, making each individual cat distinct. Domesticated cats are known for their playful, curious nature, often exhibiting behaviors such as pouncing on toys, scratching furniture, and kneading with their paws. Cats also possess exceptional hearing and vision abilities, allowing them to navigate and hunt with ease in the dark. Despite their independent reputation, many cats form strong bonds with their owners, enjoying cuddles, playtime, and even sharing meals.\\n\\n**Conclusion**\\n\\nCats continue to fascinate and enchant people around the world, offering a unique combination of independence and affection. Whether as pets, companions, or simply admired for their beauty and agility, cats have earned their place as beloved animals in human society. With their rich history, diverse breeds, and endearing personalities, it's no wonder that cats remain one of the most popular pets globally, bringing joy and companionship to millions of people worldwide.\", expected_output=None, context=None, retrieval_context=None)], confident_link=None)\n\n\n\nprompt = \"summarize the facts about cats\"\nresponse = \"\"\"Cats are popular domesticated animals known for their agility, playfulness, and affectionate nature. They have a unique physical appearance, with characteristics such as whiskers, pointy ears, and retractable claws.\nCats are also highly adaptable and can thrive in various environments, from apartments to large homes with yards. Their self-cleaning habits and low-maintenance lifestyle make them an attractive pet for many people. In conclusion, cats are fascinating animals that have become a beloved companion for millions of people around the world. Their unique appearance, playful personalities, and adaptability make them a popular choice as pets.\n\"\"\"\nprint(response)\n\nCats are popular domesticated animals known for their agility, playfulness, and affectionate nature. They have a unique physical appearance, with characteristics such as whiskers, pointy ears, and retractable claws.\nCats are also highly adaptable and can thrive in various environments, from apartments to large homes with yards. Their self-cleaning habits and low-maintenance lifestyle make them an attractive pet for many people. In conclusion, cats are fascinating animals that have become a beloved companion for millions of people around the world. Their unique appearance, playful personalities, and adaptability make them a popular choice as pets.\n\n\n\n\ndataset = EvaluationDataset(\n    test_cases=[LLMTestCase(input=prompt, actual_output=response)]\n)\nevaluate(dataset, [metric])\n\n‚ú® You're running DeepEval's latest Summarization (DAG) Metric! (using deepseek-r1:8b (Ollama), strict=False, \nasync_mode=True)...\n\n\n\nEvaluating 1 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (1/1) [Time Taken: 00:10, 10.46s/test case]\n\n\n\n======================================================================\n\nMetrics Summary\n\n  - ‚úÖ Summarization (DAG) (score: 1.0, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 1.0 because all summary headings are present and correctly ordered as 'intro', 'body', and 'conclusion'., error: None)\n\nFor test case:\n\n  - input: summarize the facts about cats\n  - actual output: Cats are popular domesticated animals known for their agility, playfulness, and affectionate nature. They have a unique physical appearance, with characteristics such as whiskers, pointy ears, and retractable claws.\nCats are also highly adaptable and can thrive in various environments, from apartments to large homes with yards. Their self-cleaning habits and low-maintenance lifestyle make them an attractive pet for many people. In conclusion, cats are fascinating animals that have become a beloved companion for millions of people around the world. Their unique appearance, playful personalities, and adaptability make them a popular choice as pets.\n\n  - expected output: None\n  - context: None\n  - retrieval context: None\n\n======================================================================\n\nOverall Metric Pass Rates\n\nSummarization (DAG): 100.00% pass rate\n\n======================================================================\n\n\n\n\n\n\n‚úì Tests finished üéâ! Run 'deepeval login' to save and analyze evaluation results on Confident AI.\n \n‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use Confident AI to get & share testing reports, \nexperiment with models/prompts, and catch regressions for your LLM system. Just run 'deepeval login' in the CLI. \n\n\n\n\nEvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Summarization (DAG)', threshold=0.5, success=True, score=1.0, reason=\"The score is 1.0 because all summary headings are present and correctly ordered as 'intro', 'body', and 'conclusion'.\", strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs=\"______________________\\n| TaskNode | Level == 0 |\\n*******************************\\nLabel: None\\n\\nInstructions:\\nExtract all headings in `actual_output`\\n\\nSummary headings:\\nCats are popular domesticated animals known for their agility, playfulness, and affectionate nature. They have a unique physical appearance, with characteristics such as whiskers, pointy ears, and retractable claws.\\n \\n \\n__________________________________\\n| BinaryJudgementNode | Level == 1 |\\n************************************************\\nLabel: None\\n\\nCriteria:\\nDoes the summary headings contain all three: 'intro', 'body', and 'conclusion'?\\n\\nVerdict: True\\nReason: The summary headings include all three sections: introduction, body, and conclusion.\\n \\n \\n_____________________________________\\n| NonBinaryJudgementNode | Level == 2 |\\n*****************************************************\\nLabel: None\\n\\nCriteria:\\nAre the summary headings in the correct order: 'intro' =&gt; 'body' =&gt; 'conclusion'?\\n\\nVerdict: Yes\\nReason: The summary headings provided are not in the correct order. The introduction should come first, followed by the body, and then the conclusion.\\n \\n \\n________________________\\n| VerdictNode | Level == 3 |\\n**********************************\\nVerdict: Yes\\nType: Deterministic\")], conversational=False, multimodal=False, input='summarize the facts about cats', actual_output='Cats are popular domesticated animals known for their agility, playfulness, and affectionate nature. They have a unique physical appearance, with characteristics such as whiskers, pointy ears, and retractable claws.\\nCats are also highly adaptable and can thrive in various environments, from apartments to large homes with yards. Their self-cleaning habits and low-maintenance lifestyle make them an attractive pet for many people. In conclusion, cats are fascinating animals that have become a beloved companion for millions of people around the world. Their unique appearance, playful personalities, and adaptability make them a popular choice as pets.\\n', expected_output=None, context=None, retrieval_context=None)], confident_link=None)",
    "crumbs": [
      "05 Dag"
    ]
  }
]