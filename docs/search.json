[
  {
    "objectID": "04_deepeval.html",
    "href": "04_deepeval.html",
    "title": "python applied machine learning",
    "section": "",
    "text": "!uv run deepeval set-ollama deepseek-r1:8b\n\nüôå Congratulations! You're now using a local Ollama model for all evals that \nrequire an LLM.\n\n\n\n!cat .deepeval\n\n{\"LOCAL_MODEL_NAME\": \"deepseek-r1:8b\", \"LOCAL_MODEL_BASE_URL\": \"http://localhost:11434\", \"USE_LOCAL_MODEL\": \"YES\", \"USE_AZURE_OPENAI\": \"NO\", \"LOCAL_MODEL_API_KEY\": \"ollama\"}\n\n\n\nfrom deepeval import evaluate\nfrom deepeval.dataset import EvaluationDataset\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCase, LLMTestCaseParams\n\ncorrectness_metric = GEval(\n    name=\"Correctness\",\n    criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\",\n    evaluation_params=[\n        LLMTestCaseParams.ACTUAL_OUTPUT,\n        LLMTestCaseParams.EXPECTED_OUTPUT,\n    ],\n    threshold=0.5,\n)\ntest_case = LLMTestCase(\n    input=\"I have a persistent cough and fever. Should I be worried?\",\n    # Replace this with the actual output of your LLM application\n    actual_output=\"A persistent cough and fever could be a viral infection or something more serious. See a doctor if symptoms worsen or don‚Äôt improve in a few days.\",\n    expected_output=\"A persistent cough and fever could indicate a range of illnesses, from a mild viral infection to more serious conditions like pneumonia or COVID-19. You should seek medical attention if your symptoms worsen, persist for more than a few days, or are accompanied by difficulty breathing, chest pain, or other concerning signs.\",\n)\n\ndataset = EvaluationDataset(test_cases=[test_case])\nevaluate(dataset, [correctness_metric])\n\n‚ú® You're running DeepEval's latest Correctness (GEval) Metric! (using deepseek-r1:8b (Ollama), strict=False, \nasync_mode=True)...\n\n\n\nEvaluating 1 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (1/1) [Time Taken: 00:42, 42.55s/test case]\n\n\n\n======================================================================\n\nMetrics Summary\n\n  - ‚úÖ Correctness (GEval) (score: 0.7, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The actual output acknowledges that persistent cough and fever could be serious but does not specify conditions like pneumonia or COVID-19. It also advises seeing a doctor if symptoms worsen or improve within a few days, which is less detailed than the expected output., error: None)\n\nFor test case:\n\n  - input: I have a persistent cough and fever. Should I be worried?\n  - actual output: A persistent cough and fever could be a viral infection or something more serious. See a doctor if symptoms worsen or don‚Äôt improve in a few days.\n  - expected output: A persistent cough and fever could indicate a range of illnesses, from a mild viral infection to more serious conditions like pneumonia or COVID-19. You should seek medical attention if your symptoms worsen, persist for more than a few days, or are accompanied by difficulty breathing, chest pain, or other concerning signs.\n  - context: None\n  - retrieval context: None\n\n======================================================================\n\nOverall Metric Pass Rates\n\nCorrectness (GEval): 100.00% pass rate\n\n======================================================================\n\n\n\n\n\n\n‚úì Tests finished üéâ! Run 'deepeval login' to save and analyze evaluation results on Confident AI.\n \n‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use Confident AI to get & share testing reports, \nexperiment with models/prompts, and catch regressions for your LLM system. Just run 'deepeval login' in the CLI. \n\n\n\n\nEvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Correctness (GEval)', threshold=0.5, success=True, score=0.7, reason='The actual output acknowledges that persistent cough and fever could be serious but does not specify conditions like pneumonia or COVID-19. It also advises seeing a doctor if symptoms worsen or improve within a few days, which is less detailed than the expected output.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Criteria:\\nDetermine if the \\'actual output\\' is correct based on the \\'expected output\\'. \\n \\nEvaluation Steps:\\n[\\n    \"Compare each aspect of the actual output with the corresponding expected output criterion.\",\\n    \"Evaluate whether the actual output meets the key requirements outlined in the expected output.\",\\n    \"Assess the quality, clarity, and effectiveness of the actual output in alignment with the criteria.\",\\n    \"Identify any deviations from the expected output and determine their impact on correctness.\"\\n]')], conversational=False, multimodal=False, input='I have a persistent cough and fever. Should I be worried?', actual_output='A persistent cough and fever could be a viral infection or something more serious. See a doctor if symptoms worsen or don‚Äôt improve in a few days.', expected_output='A persistent cough and fever could indicate a range of illnesses, from a mild viral infection to more serious conditions like pneumonia or COVID-19. You should seek medical attention if your symptoms worsen, persist for more than a few days, or are accompanied by difficulty breathing, chest pain, or other concerning signs.', context=None, retrieval_context=None)], confident_link=None)",
    "crumbs": [
      "04 Deepeval"
    ]
  },
  {
    "objectID": "02-evaluate.html",
    "href": "02-evaluate.html",
    "title": "Combining several evaluations",
    "section": "",
    "text": "import evaluate\naccuracy = evaluate.load('accuracy')\nprint(accuracy.description)\n\n\nAccuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\nAccuracy = (TP + TN) / (TP + TN + FP + FN)\n Where:\nTP: True positive\nTN: True negative\nFP: False positive\nFN: False negative\nprint(accuracy.features)\n\n{'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}\npredictions = [1, 1, 1, 1]\nreferences = [1, 1, 0, 1]\n\naccuracy.compute(predictions=predictions, references=references)\n\n{'accuracy': 0.75}\nfor pred, ref in zip(predictions, references):\n    accuracy.add(predictions=pred, references=ref)\naccuracy.compute()\n\n{'accuracy': 0.75}\nfor pred, ref in zip([[1, 1], [1, 1]], [[1, 1], [0, 1]]):\n    accuracy.add_batch(predictions=pred, references=ref)\naccuracy.compute()\n\n{'accuracy': 0.75}\nclf_metrics = evaluate.combine(['accuracy', 'f1', 'precision', 'recall'])\nclf_metrics.compute(predictions=predictions, references=references)\n\n{'accuracy': 0.75, 'f1': 0.8571428571428571, 'precision': 0.75, 'recall': 1.0}",
    "crumbs": [
      "Combining several evaluations"
    ]
  },
  {
    "objectID": "02-evaluate.html#rouge",
    "href": "02-evaluate.html#rouge",
    "title": "Combining several evaluations",
    "section": "1 Rouge",
    "text": "1 Rouge\nhttps://huggingface.co/spaces/evaluate-metric/rouge\n\nrouge = evaluate.load('rouge')\nprint(rouge.description)\n\nROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for\nevaluating automatic summarization and machine translation software in natural language processing.\nThe metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\n\nNote that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters.\n\nThis metrics is a wrapper around Google Research reimplementation of ROUGE:\nhttps://github.com/google-research/google-research/tree/master/rouge\n\n\n\n\nprint(rouge.features)\n\n[{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}]\n\n\n\nrouge.compute(predictions=['hello john', 'I am one'], \n              references=['hello John', 'I am two'])\n\n{'rouge1': 0.8333333333333333,\n 'rouge2': 0.75,\n 'rougeL': 0.8333333333333333,\n 'rougeLsum': 0.8333333333333333}\n\n\n\nrouge.compute(predictions=['the cat sat on the mat'], \n              references=['the cat was on the mat'])\n\n{'rouge1': 0.8333333333333334,\n 'rouge2': 0.6,\n 'rougeL': 0.8333333333333334,\n 'rougeLsum': 0.8333333333333334}",
    "crumbs": [
      "Combining several evaluations"
    ]
  },
  {
    "objectID": "02-evaluate.html#bleu",
    "href": "02-evaluate.html#bleu",
    "title": "Combining several evaluations",
    "section": "2 BLEU",
    "text": "2 BLEU\nhttps://huggingface.co/spaces/evaluate-metric/bleu\n\nbleu = evaluate.load('bleu')\nprint(bleu.description)\n\nDownloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.94k/5.94k [00:00&lt;00:00, 5.88MB/s]\nDownloading extra modules: 4.07kB [00:00, 7.39MB/s]                                                                                                    \nDownloading extra modules: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.34k/3.34k [00:00&lt;00:00, 10.3MB/s]\n\n\nBLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another.\nQuality is considered to be the correspondence between a machine's output and that of a human: \"the closer a machine translation is to a professional human translation, the better it is\"\n‚Äì this is the central idea behind BLEU. BLEU was one of the first metrics to claim a high correlation with human judgements of quality, and remains one of the most popular automated and inexpensive metrics.\n\nScores are calculated for individual translated segments‚Äîgenerally sentences‚Äîby comparing them with a set of good quality reference translations.\nThose scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality.\nNeither intelligibility nor grammatical correctness are not taken into account.\n\n\n\n\n\n\n\nprint(bleu.features)\n\n[{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}]\n\n\n\nbleu.compute(predictions=['hello john', 'I am one'], \n             references=['hello John', 'I am two'])\n\n{'bleu': 0.0,\n 'precisions': [0.6, 0.3333333333333333, 0.0, 0.0],\n 'brevity_penalty': 1.0,\n 'length_ratio': 1.0,\n 'translation_length': 5,\n 'reference_length': 5}\n\n\n\nbleu.compute(predictions=['the cat sat on the mat'], \n             references=['the cat was on the mat'])\n\n{'bleu': 0.0,\n 'precisions': [0.8333333333333334, 0.6, 0.25, 0.0],\n 'brevity_penalty': 1.0,\n 'length_ratio': 1.0,\n 'translation_length': 6,\n 'reference_length': 6}",
    "crumbs": [
      "Combining several evaluations"
    ]
  },
  {
    "objectID": "01-eval.html",
    "href": "01-eval.html",
    "title": "Evaluating LLM Performance",
    "section": "",
    "text": "To effectively evaluate LLM performance, we need to consider various aspects beyond just accuracy. These aspects include fluency, coherence, relevance, diversity, factual accuracy, and the ability to generate meaningful responses.\n\nLLM Benchmarks: Example: SuperGLUE, a suite of benchmarks for evaluating LLMs on diverse natural language tasks.\nMetrics: Example: BLEU score, which measures the similarity between generated text and human-written references.\nHuman Evaluation: Example: Collect human judgments on the quality of generated text, such as fluency, coherence, and informativeness.\nMulti-Task Evaluation: Example: Evaluate an LLM on a variety of tasks to assess its overall effectiveness.\nExplainability Methods: Example: Utilize techniques such as attention visualization to understand how an LLM generates text and identify potential biases.\n\nBLEU and ROUGE scores are metrics commonly used to evaluate the performance of natural language processing (NLP) models, specifically machine translation and text summarization models.\n\n0.1 BLEU (Bilingual Evaluation Understudy):\n\nMeasures the similarity between a machine-generated text and human-written reference translations.\nFocuses on precision, meaning it penalizes the model for generating words not present in the reference translations.\nCalculates n-gram matches (overlapping sequences of words) between the generated and reference texts.\nRanges from 0 (no overlap) to 1 (perfect match).\n\n\n\n0.2 ROUGE (Recall-Oriented Understudy for Gisting Evaluation):\n\nMeasures the overlap between machine-generated summaries and human-written reference summaries.\nFocuses on recall, meaning it rewards the model for capturing important information from the original text.\nUtilizes various n-gram sizes to capture different levels of granularity.\nOffers several variants, including ROUGE-N (n-gram overlap), ROUGE-L (longest common subsequence), and ROUGE-W (weighted n-gram overlap).\nScores are reported as F1 scores, which combine precision and recall.\n\nBoth BLEU and ROUGE scores offer valuable insights into the performance of NLP models, but they have limitations. BLEU can be sensitive to word order and may not capture fluency or coherence. ROUGE can reward summaries that are overly repetitive or lack originality.\nHere are some additional points to consider:\n\nBLEU is often used for machine translation, while ROUGE is preferred for text summarization.\nNewer metrics are being developed to address the limitations of BLEU and ROUGE, such as METEOR and CHRF++.\nHuman evaluation is still considered the gold standard for NLP model evaluation, but it can be expensive and time-consuming.\n\nNo single metric can perfectly capture the quality of a machine-generated text. It‚Äôs important to consider the specific task and application when choosing the most appropriate evaluation methods.",
    "crumbs": [
      "Evaluating LLM Performance"
    ]
  },
  {
    "objectID": "03-llm.html",
    "href": "03-llm.html",
    "title": "Evaluating LLM using LLM",
    "section": "",
    "text": "LLMs can be used to evaluate other LLMs by leveraging their ability to generate text, translate languages, and answer questions. This approach offers several potential benefits, including:\n\nScalability: LLMs can quickly evaluate large datasets of text, overcoming the time and resource limitations of human evaluation.\nObjectivity: LLMs can be less prone to bias than human evaluators, potentially leading to more consistent and fair assessments.\nFlexibility: LLMs can be tailored to specific evaluation tasks and criteria by providing appropriate prompts and instructions.\n\nHere are some sample prompts for evaluating an LLM using another LLM:\n\n0.1 1. Fluency and Coherence:\n\nPrompt: ‚ÄúRead the following text and rate its fluency and coherence on a scale of 1 to 5. Provide specific feedback on areas for improvement.‚Äù\nExample: ‚ÄúThe cat sat on the mat. The dog chased the ball. The sun was shining brightly.‚Äù\n\n\n\n0.2 2. Relevance and Informativeness:\n\nPrompt: ‚ÄúWrite a summary of the following text and assess its relevance and informativeness. Does it capture the key points of the original text effectively?‚Äù\nExample: ‚ÄúThe article discusses the history of artificial intelligence and its potential impact on society.‚Äù\n\n\n\n0.3 3. Factual Accuracy and Credibility:\n\nPrompt: ‚ÄúVerify the factual accuracy of the following claims and indicate any sources that support your findings.‚Äù\nExample: ‚ÄúClimate change is caused by human activity.‚Äù\n\n\n\n0.4 4. Originality and Creativity:\n\nPrompt: ‚ÄúGenerate a creative text format based on the following prompt, demonstrating originality and effective use of language.‚Äù\nExample: ‚ÄúWrite a poem about a lost love.‚Äù\n\n\n\n0.5 5. Societal and Ethical Considerations:\n\nPrompt: ‚ÄúAnalyze the following text for potential biases or harmful stereotypes. Suggest ways to mitigate these issues.‚Äù\nExample: ‚ÄúA news article reporting on a crime that uses discriminatory language.‚Äù",
    "crumbs": [
      "Evaluating LLM using LLM"
    ]
  }
]